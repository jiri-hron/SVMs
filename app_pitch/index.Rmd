---
title       : Support Vector Machines - interactive app
subtitle    : Try tuning SVM - watch how it behaves in different settings
author      : Jiri Hron
framework   : revealjs      # {io2012, html5slides, shower, dzslides, ...}
highlighter : highlight.js  # {highlight.js, prettify, highlight}
widgets     : [mathjax]     # {mathjax, quiz, bootstrap}
mode        : standalone # {standalone, draft}
knit        : slidify::knit2slides
---

## Support Vector Machines interactive
by Jiri Hron

---

## Why SVM?
<br>
  1. Currently one of the most popular algorithms among Machine Learning
  community due to its effectivness
  2. Can be used both in classification and regression settings
  3. Elegant algorithm with non-trivial implementation and tuning

---

## Who can be interested?

<br>

  1. Teachers - to demonstrate in real-time how SVMs work with different tuning
  parameters and settings
  2. Students - who are currently studying SVMs and visual demonstrations
  help them understand
  3. Geeks - who have two minutes of time and are keen on machine learning, 
  algorithms or statistics

---

```{r echo=FALSE, warning=FALSE, message=FALSE, fig.align='center', fig.height=7, fig.width=7}
if(!require(e1071)) {
  stop("install e1071 package")
}

## generate demo data
set.seed(132433)
x01 = rnorm(100, mean = 1, sd = .5)
x02 = rnorm(100, mean = 1, sd = .5)
x12 = rnorm(100, mean = 4, sd = .6)
x22 = rnorm(100, mean = 4, sd = .6)
df = data.frame(x1 = c(x01,x12), x2 = c(x02, x22))

## add response variable
y = rep("A", 200)
y[101:200] = "B"
x = as.matrix(df)
df$cat = as.factor(y)

## fit svm model
fit.svm = svm(cat ~ .,
              data = df,
              kernel = "linear",
              cost = 10,
              scale = FALSE,
              type = "C-classification")

## make grid of all points on the graph in range of generated clusters and
## predict category for each of the points on the grid
source("../src/make.grid.R", local = TRUE)
xgrid = make.grid(x)
ygrid = predict(fit.svm, xgrid)
df.grid = data.frame(x1 = xgrid$x1, x2 = xgrid$x2, cat = ygrid)

## get coeffiecients used for classification of each of the observations on
## the grid; will be used on the plot to visualize decision boundary and
## the associated margin
beta = drop(t(fit.svm$coefs) %*% as.matrix(df[fit.svm$index, -3]))
beta0 = fit.svm$rho


## plot the grid to show separation done by hyperplane found by SVM
par(bg = "#636363", fg = "white", family = "sans")
plot(xgrid, col = c("#66C2A5", "#FC8D62")[as.numeric(ygrid)],
     pch = 20, cex = 0.2,
     col.axis = "white",
     col.lab = "white",
     col.main = "white",
     col.sub = "white")

## plot generated data set
points(df[,-3],
       col = c("#66C2A5", "#FC8D62")[as.numeric(df[,3])],
       pch = 19, cex = 1.5)

## highlight support vectors by drawing rectangles around them
points(df[fit.svm$index, -3], pch = 5, cex = 2.5, col = "white")

## plot the decision boundary and margin
abline(beta0/beta[2], -beta[1]/beta[2], lwd = 2, col = "white")
abline((beta0 - 1)/beta[2], -beta[1]/beta[2], lty = 2, col = "white")
abline((beta0 + 1)/beta[2], -beta[1]/beta[2], lty = 2, col = "white")

```

<br>

Application enables any user to quickly generate a data set and fit SVM to it 
with readily available possibility to try-out diffetent tuning parameters.

---

## Application capabilities

<p style="text-align: left">Based on this formula:</p>

$$min_\theta C \sum_{i=1}^m \Big[ y^{(i)} cost_1(\theta^T f^{(i)})
    + (1 - y^{(i)}) cost_0 (\theta^T f^{(i)}) \Big]$$
    $$+ \frac{1}{2} \sum_{i=1}^n \theta_j^2$$ 
    
<p style="text-align: left">where $cost_1$ and $cost_0$ are 
corresponding Hinge loss functions for given class you can:</p>

  * use Linear or Gaussian kernel function (here $f^{(i)}$)
  * adjust parameter $C$
  * change $\sigma$ when using Gaussian kernel
  * generate custom data set with two groups and train a new SVM instance on it

